{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10f1df44",
   "metadata": {},
   "source": [
    "# Assignment 1: Star Schema/ Snowflake Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042c1740",
   "metadata": {},
   "source": [
    "1)Created required dimension tables(Products, Brand, Categories, ProductVariants, Customers, Addresses, Prices)\n",
    "2)Created Sales fact table\n",
    "3)To accommodate the changing nature of customer details, product variants, and prices over time while retaining historical information i adjusted our schema to supprot SCD\n",
    "and add additional tables for product variants and price history\n",
    "4)Created below script to create and insert sample data using faker function to generate random fake products for analysis\n",
    "5)Stored the schema and data to Mysql Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09316892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables created successfully.\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "from faker import Faker\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize Faker to create fake data\n",
    "fake = Faker()\n",
    "\n",
    "# Connect to MySQL database\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"127.0.0.1\",\n",
    "    user=\"root\",\n",
    "    password=\"Nishant@9372\",\n",
    "    database=\"REUNION2\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create dimension tables\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS CategoriesDim (\n",
    "                    CategoryID INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                    CategoryName VARCHAR(255)\n",
    "                )''')\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS ProductsDim (\n",
    "                    ProductID INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                    ProductName VARCHAR(255),\n",
    "                    Description TEXT,\n",
    "                    CategoryID INT,\n",
    "                    FOREIGN KEY (CategoryID) REFERENCES CategoriesDim(CategoryID)\n",
    "                )''')\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS ProductVariantsDim (\n",
    "                    VariantID INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                    ProductID INT,\n",
    "                    VariantName VARCHAR(255),\n",
    "                    Attributes TEXT,\n",
    "                    CurrentPriceID INT,\n",
    "                    LaunchDate DATE,\n",
    "                    DiscontinueDate DATE,\n",
    "                    CurrentFlag BOOLEAN,\n",
    "                    FOREIGN KEY (ProductID) REFERENCES ProductsDim(ProductID)\n",
    "                )''')\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS CustomersDim (\n",
    "                    CustomerID INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                    FirstName VARCHAR(255),\n",
    "                    LastName VARCHAR(255),\n",
    "                    Gender VARCHAR(50),\n",
    "                    DateOfBirth DATE,\n",
    "                    Email VARCHAR(255),\n",
    "                    Phone VARCHAR(50),\n",
    "                    CurrentAddressID INT,\n",
    "                    StartDate DATE,\n",
    "                    EndDate DATE,\n",
    "                    CurrentFlag BOOLEAN\n",
    "                )''')\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS AddressesDim (\n",
    "                    AddressID INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                    CustomerID INT,\n",
    "                    StreetAddress VARCHAR(255),\n",
    "                    City VARCHAR(255),\n",
    "                    State VARCHAR(255),\n",
    "                    Country VARCHAR(255),\n",
    "                    ZipCode VARCHAR(20),\n",
    "                    StartDate DATE,\n",
    "                    EndDate DATE,\n",
    "                    FOREIGN KEY (CustomerID) REFERENCES CustomersDim(CustomerID)\n",
    "                )''')\n",
    "\n",
    "\n",
    "\n",
    "# Create Fact table\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS OrdersFact (\n",
    "                    OrderID INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                    CustomerID INT,\n",
    "                    ProductVariantID INT,\n",
    "                    OrderDate DATE,\n",
    "                    OrderQuantity INT,\n",
    "                    TotalPrice DECIMAL(10, 2),\n",
    "                    FOREIGN KEY (CustomerID) REFERENCES CustomersDim(CustomerID),\n",
    "                    FOREIGN KEY (ProductVariantID) REFERENCES ProductVariantsDim(VariantID)\n",
    "                )''')\n",
    "\n",
    "# Commit changes and close connection\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"Tables created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "765bafe4",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "MySQL Connection not available.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26284\\1548265393.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Generate and insert sample data for CategoriesDim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcategories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Electronics'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Clothing'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Home Appliances'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcategory\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcategories\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\mysql\\connector\\connection_cext.py\u001b[0m in \u001b[0;36mcursor\u001b[1;34m(self, buffered, raw, prepared, cursor_class, dictionary, named_tuple)\u001b[0m\n\u001b[0;32m    762\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle_unread_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepared\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_connected\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mOperationalError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MySQL Connection not available.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcursor_class\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcursor_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCMySQLCursor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOperationalError\u001b[0m: MySQL Connection not available."
     ]
    }
   ],
   "source": [
    "cursor = conn.cursor()\n",
    "\n",
    "# Generate and insert sample data for CategoriesDim\n",
    "categories = ['Electronics', 'Clothing', 'Home Appliances']\n",
    "for category in categories:\n",
    "    cursor.execute('''INSERT INTO CategoriesDim (CategoryName) VALUES (%s)''', (category,))\n",
    "\n",
    "# Generate and insert sample data for BrandsDim\n",
    "brands = ['Samsung', 'Apple', 'Nike', 'Adidas', 'Sony']\n",
    "for brand in brands:\n",
    "    cursor.execute('''INSERT INTO BrandsDim (BrandName) VALUES (%s)''', (brand,))\n",
    "\n",
    "# Generate and insert sample data for ProductsDim\n",
    "products = []\n",
    "for _ in range(10):\n",
    "    product_name = fake.word()\n",
    "    description = fake.text()\n",
    "    category_id = random.randint(1, len(categories))\n",
    "    brand_id = random.randint(1, len(brands))\n",
    "    cursor.execute('''INSERT INTO ProductsDim (ProductName, Description, CategoryID, BrandID) VALUES (%s, %s, %s, %s)''',\n",
    "                   (product_name, description, category_id, brand_id))\n",
    "    products.append(cursor.lastrowid)\n",
    "\n",
    "# Generate and insert sample data for ProductVariantsDim\n",
    "for product_id in products[:5]:\n",
    "    for _ in range(2):\n",
    "        variant_name = fake.word()\n",
    "        attributes = fake.words(nb=5)\n",
    "        launch_date = fake.date_between(start_date='-2y', end_date='today')\n",
    "        discontinue_date = fake.date_between(start_date=launch_date, end_date='today')\n",
    "        cursor.execute('''INSERT INTO ProductVariantsDim (ProductID, VariantName, Attributes, LaunchDate, DiscontinueDate) VALUES (%s, %s, %s, %s, %s)''',\n",
    "                       (product_id, variant_name, ' '.join(attributes), launch_date, discontinue_date))\n",
    "\n",
    "# Generate and insert sample data for CustomersDim\n",
    "for _ in range(10):\n",
    "    first_name = fake.first_name()\n",
    "    last_name = fake.last_name()\n",
    "    gender = random.choice(['Male', 'Female'])\n",
    "    dob = fake.date_of_birth(minimum_age=18, maximum_age=90)\n",
    "    email = fake.email()\n",
    "    phone = fake.phone_number()\n",
    "    cursor.execute('''INSERT INTO CustomersDim (FirstName, LastName, Gender, DateOfBirth, Email, Phone) VALUES (%s, %s, %s, %s, %s, %s)''',\n",
    "                   (first_name, last_name, gender, dob, email, phone))\n",
    "\n",
    "# Generate and insert sample data for AddressesDim\n",
    "for customer_id in range(1, 11):\n",
    "    street_address = fake.street_address()\n",
    "    city = fake.city()\n",
    "    state = fake.state()\n",
    "    country = fake.country()\n",
    "    zip_code = fake.zipcode()\n",
    "    start_date = fake.date_between(start_date='-2y', end_date='today')\n",
    "    end_date = fake.date_between(start_date=start_date, end_date='today')\n",
    "    cursor.execute('''INSERT INTO AddressesDim (CustomerID, StreetAddress, City, State, Country, ZipCode, StartDate, EndDate) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)''',\n",
    "                   (customer_id, street_address, city, state, country, zip_code, start_date, end_date))\n",
    "\n",
    "# Generate and insert sample data for PricesDim\n",
    "for product_id in products[:5]:\n",
    "    for _ in range(2):\n",
    "        variant_id = random.randint(1, 10)\n",
    "        price = round(random.uniform(50, 1000), 2)\n",
    "        effective_date = fake.date_between(start_date='-2y', end_date='today')\n",
    "        end_date = fake.date_between(start_date=effective_date, end_date='today')\n",
    "        cursor.execute('''INSERT INTO PricesDim (VariantID, Price, EffectiveDate, EndDate) VALUES (%s, %s, %s, %s)''',\n",
    "                       (variant_id, price, effective_date, end_date))\n",
    "\n",
    "# Generate and insert sample data for OrdersFact\n",
    "for _ in range(500):  # 500 orders for 2 years of order history\n",
    "    customer_id = random.randint(1, 10)\n",
    "    product_variant_id = random.randint(1, 10)\n",
    "    order_date = fake.date_between(start_date='-2y', end_date='today')\n",
    "    order_quantity = random.randint(1, 5)\n",
    "    total_price = round(random.uniform(10, 500), 2)\n",
    "    cursor.execute('''INSERT INTO OrdersFact (CustomerID, ProductVariantID, OrderDate, OrderQuantity, TotalPrice)\n",
    "                      VALUES (%s, %s, %s, %s, %s)''',\n",
    "                   (customer_id, product_variant_id, order_date, order_quantity, total_price))\n",
    "\n",
    "# Commit changes and close connection\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"Sample data inserted successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6eb92c",
   "metadata": {},
   "source": [
    "# Assignment 2:SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6d4ec5",
   "metadata": {},
   "source": [
    "By taking reference of above star schema created above performed all the required query in Mysql console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8799bc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) Retrieve the top 5 customers who have made the highest average order amounts in the last 6 months. The average order amount should be calculated for each customer, and the result should be sorted in descending order.\n",
    "SELECT O.CustomerID, C.FirstName, AVG(O.TotalPrice) AS AverageOrderAmount\n",
    "FROM OrdersFact O\n",
    "Left Join customersdim C on O.CustomerID=C.CustomerID\n",
    "WHERE OrderDate >= DATE_SUB(CURRENT_DATE(), INTERVAL 6 MONTH)\n",
    "GROUP BY CustomerID\n",
    "ORDER BY AverageOrderAmount DESC\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ffaf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2)Retrieve the list of customer whose order value is lower this year as compared to previous year\n",
    "WITH cte1 AS (\n",
    "    SELECT CustomerID, SUM(TotalPrice) AS TotalOrderSum2024\n",
    "    FROM ordersfact\n",
    "    WHERE YEAR(OrderDate) = 2024\n",
    "    GROUP BY CustomerID\n",
    "),\n",
    "cte2 AS (\n",
    "    SELECT CustomerID, SUM(TotalPrice) AS TotalOrderSum2023\n",
    "    FROM ordersfact\n",
    "    WHERE YEAR(OrderDate) = 2023\n",
    "    GROUP BY CustomerID\n",
    ")\n",
    "SELECT cte1.CustomerID, c.FIRSTNAME\n",
    "FROM cte1        \n",
    "LEFT JOIN cte2 ON cte1.CustomerID = cte2.CustomerID\n",
    "LEFT JOIN CustomersDim c on c.CustomerID = cte1.CustomerID\n",
    "WHERE cte2.TotalOrderSum2023 > cte1.TotalOrderSum2024;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8998d633",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3) Create a table showing cumulative purchase by a particular customer. Show the breakup of cumulative purchases by product category\n",
    "SELECT \n",
    "    c.CustomerID,\n",
    "    CONCAT(c.FirstName, ' ', c.LastName) AS CustomerName,\n",
    "    p.CategoryID,\n",
    "    cd.CategoryName,\n",
    "    SUM(o.TotalPrice) AS CumulativePurchase\n",
    "FROM \n",
    "    CustomersDim c\n",
    "JOIN \n",
    "    OrdersFact o ON c.CustomerID = o.CustomerID\n",
    "JOIN \n",
    "    ProductVariantsDim pv ON o.ProductVariantID = pv.VariantID\n",
    "JOIN \n",
    "    ProductsDim p ON pv.ProductID = p.ProductID\n",
    "JOIN \n",
    "    CategoriesDim cd ON p.CategoryID = cd.CategoryID\n",
    "WHERE \n",
    "    c.CustomerID = 1\n",
    "GROUP BY \n",
    "    c.CustomerID, p.CategoryID\n",
    "ORDER BY \n",
    "    c.CustomerID, p.CategoryID;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8f27c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4) Retrieve the list of top 5 selling products. Further bifurcate the sales by product variants\n",
    "SELECT\n",
    "    p.ProductID,\n",
    "    p.ProductName,\n",
    "    pv.VariantID,\n",
    "    pv.VariantName,\n",
    "    SUM(o.OrderQuantity) AS TotalQuantity\n",
    "FROM\n",
    "    ProductsDim p\n",
    "JOIN\n",
    "    ProductVariantsDim pv ON p.ProductID = pv.ProductID\n",
    "JOIN\n",
    "    OrdersFact o ON pv.VariantID = o.ProductVariantID\n",
    "GROUP BY\n",
    "    p.ProductID, p.ProductName, pv.VariantID, pv.VariantName\n",
    "ORDER BY\n",
    "    SUM(o.OrderQuantity) DESC\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73618194",
   "metadata": {},
   "source": [
    "# Assignment 5: Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff19021",
   "metadata": {},
   "source": [
    "Python Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86365673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel('Time Series.xlsx')\n",
    "\n",
    "# Convert Start and End columns to datetime\n",
    "df['Start'] = pd.to_datetime(df['Start'])\n",
    "df['End'] = pd.to_datetime(df['End'])\n",
    "\n",
    "# Correct the End dates if the End date is earlier than the Start date\n",
    "df.loc[df['End'] < df['Start'], 'End'] += pd.DateOffset(years=1)\n",
    "\n",
    "# Function to merge overlapping periods and aggregate activities for a single bot\n",
    "def merge_periods_and_aggregate_activities(bot_df):\n",
    "    bot_df = bot_df.sort_values(by='Start').reset_index(drop=True)\n",
    "    \n",
    "    merged_periods = []\n",
    "    current_start = bot_df.loc[0, 'Start']\n",
    "    current_end = bot_df.loc[0, 'End']\n",
    "    current_activities = [bot_df.loc[0, 'Activity']]\n",
    "    \n",
    "    for i in range(1, len(bot_df)):\n",
    "        row = bot_df.loc[i]\n",
    "        if row['Start'] <= current_end:\n",
    "            current_end = max(current_end, row['End'])\n",
    "            current_activities.append(row['Activity'])\n",
    "        else:\n",
    "            merged_periods.append((current_start, current_end, current_activities))\n",
    "            current_start = row['Start']\n",
    "            current_end = row['End']\n",
    "            current_activities = [row['Activity']]\n",
    "    \n",
    "    merged_periods.append((current_start, current_end, current_activities))\n",
    "    \n",
    "    return merged_periods\n",
    "\n",
    "# Group by bot name and process each group\n",
    "result = defaultdict(list)\n",
    "for name, group in df.groupby('Name'):\n",
    "    result[name] = merge_periods_and_aggregate_activities(group)\n",
    "\n",
    "# Convert the result to a structured format\n",
    "output = []\n",
    "for name, periods in result.items():\n",
    "    for start, end, activities in periods:\n",
    "        output.append({'Name': name, 'Start': start, 'End': end, 'Activities': activities})\n",
    "\n",
    "# Convert the output to a DataFrame\n",
    "output_df = pd.DataFrame(output)\n",
    "\n",
    "# Save to a new Excel file\n",
    "output_df.to_excel('output.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e044bca4",
   "metadata": {},
   "source": [
    "SQL Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38df32e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "WITH RECURSIVE WorkPeriods AS (\n",
    "    SELECT \n",
    "        Name,\n",
    "        Start,\n",
    "        End,\n",
    "        Activity,\n",
    "        ROW_NUMBER() OVER (PARTITION BY Name ORDER BY Start) AS rn\n",
    "    FROM agi_tasks\n",
    "),\n",
    "MergedPeriods AS (\n",
    "    SELECT\n",
    "        wp1.Name,\n",
    "        wp1.Start,\n",
    "        wp1.End,\n",
    "        wp1.Activity\n",
    "    FROM WorkPeriods wp1\n",
    "    WHERE wp1.rn = 1\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        wp2.Name,\n",
    "        LEAST(mp.Start, wp2.Start),\n",
    "        GREATEST(mp.End, wp2.End),\n",
    "        mp.Activity || ', ' || wp2.Activity\n",
    "    FROM MergedPeriods mp\n",
    "    JOIN WorkPeriods wp2 ON mp.Name = wp2.Name AND wp2.rn = (SELECT rn + 1 FROM WorkPeriods WHERE Name = wp2.Name AND Start = mp.Start)\n",
    "    WHERE wp2.Start <= mp.End\n",
    "),\n",
    "FinalPeriods AS (\n",
    "    SELECT\n",
    "        Name,\n",
    "        Start,\n",
    "        End,\n",
    "        Activity,\n",
    "        ROW_NUMBER() OVER (PARTITION BY Name, Start ORDER BY End DESC) AS rn\n",
    "    FROM MergedPeriods\n",
    ")\n",
    "SELECT \n",
    "    Name,\n",
    "    MIN(Start) AS Start,\n",
    "    MAX(End) AS End,\n",
    "    Activity\n",
    "FROM FinalPeriods\n",
    "WHERE rn = 1\n",
    "GROUP BY Name, Activity\n",
    "ORDER BY Name, Start;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903e9e7d",
   "metadata": {},
   "source": [
    "# Assignment 4: Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f4ecf3",
   "metadata": {},
   "source": [
    "1)Firstly Created Azure storage Account\n",
    "2)Navigated to storage account and created new container\n",
    "3)Created a Databricks workspace and written the below code at notebook\n",
    "4)The below code writes the ingested data directly to a Delta table in the specified Azure Blob Storage path without performing any transformations.\n",
    "5)The cloudFiles.inferColumnTypes option in Autoloader automatically infers the schema of the ingested data.\n",
    "6)Databricks Autoloader automatically detects new files added to the specified Azure Blob Storage path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d60784ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3341294003.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\nisha\\AppData\\Local\\Temp\\ipykernel_26284\\3341294003.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    pip install pyspark\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Set up the configurations\n",
    "storage_account_name = \"newnishantstorageaccount\"\n",
    "container_name = \"reunionassignment\"\n",
    "sas_token = \"sp=r&st=2024-06-13T07:08:21Z&se=2024-06-13T15:08:21Z&spr=https&sv=2022-11-02&sr=c&sig=BIXwpx9R0JOdeA2P%2FI4hS1s16KLC%2FPXDnCayKkQZNKY%3D\"\n",
    "# Define the input and schema paths\n",
    "input_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/?{sas_token}\"\n",
    "schema_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/schema\"\n",
    "\n",
    "# Configure Autoloader\n",
    "df = spark.readStream.format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"json\") \\\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\") \\\n",
    "    .option(\"cloudFiles.schemaLocation\", schema_path) \\\n",
    "    .load(input_path)\n",
    "\n",
    "# Write data to a Delta table\n",
    "output_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/output_table\"\n",
    "\n",
    "query = df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/checkpoints\") \\\n",
    "    .start(output_path)\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510b9aa3",
   "metadata": {},
   "source": [
    "# Assignment 3: ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05525863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import urllib.parse\n",
    "\n",
    "# Load JSON data\n",
    "with open('Reunion_nested_data.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Initialize lists for each table\n",
    "programs_list = []\n",
    "concerts_list = []\n",
    "works_list = []\n",
    "soloists_list = []\n",
    "composers_set = set()\n",
    "\n",
    "# Parse the JSON data and populate the lists\n",
    "for program in data['programs']:\n",
    "    program_id = program['programID']\n",
    "    season = program['season']\n",
    "    orchestra = program['orchestra']\n",
    "    \n",
    "    programs_list.append({'ProgramID': program_id, 'Season': season, 'Orchestra': orchestra})\n",
    "    \n",
    "    for concert in program['concerts']:\n",
    "        concerts_list.append({'ProgramID': program_id, 'Date': concert['Date'], 'EventType': concert['eventType'], 'Venue': concert['Venue'], 'Location': concert['Location'], 'Time': concert['Time']})\n",
    "    \n",
    "    for work in program['works']:\n",
    "        work_id = work['ID']\n",
    "        title = work.get('workTitle', None)\n",
    "        conductor = work.get('conductorName', None)\n",
    "        composer = work.get('composerName', None)  # Use .get() to handle missing 'composerName'\n",
    "        movement = work.get('movement', None)\n",
    "        interval = work.get('interval', None)\n",
    "        \n",
    "        works_list.append({'WorkID': work_id, 'ProgramID': program_id, 'Title': title, 'ConductorName': conductor, 'Composer': composer, 'Movement': movement, 'Interval': interval})\n",
    "        \n",
    "        # Add composer to the set\n",
    "        if composer:\n",
    "            composers_set.add(composer)\n",
    "        \n",
    "        for soloist in work.get('soloists', []):  # Use .get() to handle missing 'soloists'\n",
    "            soloists_list.append({'SoloistName': soloist['soloistName'], 'Roles': soloist['soloistRoles'], 'Instrument': soloist['soloistInstrument'], 'WorkID': work_id})\n",
    "\n",
    "# Convert the sets to lists\n",
    "composers_list = [{'Name': composer} for composer in composers_set]\n",
    "\n",
    "# Create dataframes\n",
    "programs_df = pd.DataFrame(programs_list)\n",
    "concerts_df = pd.DataFrame(concerts_list)\n",
    "works_df = pd.DataFrame(works_list)\n",
    "soloists_df = pd.DataFrame(soloists_list)\n",
    "composers_df = pd.DataFrame(composers_list)\n",
    "\n",
    "# MySQL connection details\n",
    "mysql_user = 'root'\n",
    "mysql_password = urllib.parse.quote_plus('Nishant@9372')  # Encode the password\n",
    "mysql_host = '127.0.0.1'\n",
    "mysql_db = 'orchestra'\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(f'mysql+mysqlconnector://{mysql_user}:{mysql_password}@{mysql_host}/{mysql_db}')\n",
    "\n",
    "# Insert data into works table row by row\n",
    "for index, row in works_df.iterrows():\n",
    "    insert_query = \"INSERT INTO works (`WorkID`, `ProgramID`, `Title`, `ConductorName`, `Composer`, `Movement`, `Interval`) VALUES (%s, %s, %s, %s, %s, %s, %s)\"\n",
    "    engine.execute(insert_query, tuple(row))\n",
    "\n",
    "# Save other dataframes to the MySQL database\n",
    "programs_df.to_sql('programs', engine, if_exists='replace', index=False)\n",
    "concerts_df.to_sql('concerts', engine, if_exists='replace', index=False)\n",
    "soloists_df.to_sql('soloists', engine, if_exists='replace', index=False)\n",
    "composers_df.to_sql('composers', engine, if_exists='replace', index=False)\n",
    "\n",
    "# Print DataFrames for verification\n",
    "print(programs_df)\n",
    "print(concerts_df)\n",
    "print(soloists_df)\n",
    "print(composers_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
